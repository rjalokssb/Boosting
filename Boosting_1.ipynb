{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is boosting in machine learning?"
      ],
      "metadata": {
        "id": "R_b0qF6Jwvru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTD5L9BPwssd"
      },
      "outputs": [],
      "source": [
        "# Boosting is a popular machine learning technique used for improving the accuracy of weak learners by combining them into a stronger ensemble model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the advantages and limitations of using boosting techniques?"
      ],
      "metadata": {
        "id": "PI_xeoULww3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Advantages: (Improved accuracy, Robustness, Flexibility, Automatic feature selection, Reduced overfitting)\n",
        "# Limitations : (Sensitivity to outliers, Computationally expensive, Risk of model collapse, Requires a large amount of data)"
      ],
      "metadata": {
        "id": "piGu2sTTw0hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explain how boosting works."
      ],
      "metadata": {
        "id": "vIZ_GeEKw05t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the weights -> Build a weak learner -> Evaluate the weak learner -> Update the weights -> Build another weak learner -> Combine the weak learners -> Repeat the process"
      ],
      "metadata": {
        "id": "71EPFgpuw2r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are the different types of boosting algorithms?"
      ],
      "metadata": {
        "id": "xnVcKZwyw3Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost \n",
        "# Gradient Boosting\n",
        "# XGBoost "
      ],
      "metadata": {
        "id": "CtTqpR21w5kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are some common parameters in boosting algorithms?"
      ],
      "metadata": {
        "id": "MDPqtuxHw5_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of estimators\n",
        "# Learning rate\n",
        "# Max depth\n",
        "# Regularization"
      ],
      "metadata": {
        "id": "IKUvg93jw8IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
      ],
      "metadata": {
        "id": "11tL4EOvw8hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the weights -> Build a weak learner -> Evaluate the weak learner -> Update the weights -> Build another weak learner -> Combine the weak learners -> Repeat the process"
      ],
      "metadata": {
        "id": "TKUoNRGXw_Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Explain the concept of AdaBoost algorithm and its working."
      ],
      "metadata": {
        "id": "8AZfZMmbw_82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost (Adaptive Boosting) is a popular boosting algorithm that is commonly used in machine learning for classification tasks.\n",
        "# Initialize weights\n",
        "# Build a weak learner\n",
        "# Evaluate the weak learner\n",
        "# Update the weights\n",
        "# Repeat: Steps 2 to 4 \n",
        "# Combine the weak learners\n",
        "# Assign weights to weak learners\n",
        "# Final prediction"
      ],
      "metadata": {
        "id": "Sis2fxcaxCDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is the loss function used in AdaBoost algorithm?"
      ],
      "metadata": {
        "id": "nOk8HccbxCSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost algorithm uses the exponential loss function,"
      ],
      "metadata": {
        "id": "Xyz1kh9FxEb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
      ],
      "metadata": {
        "id": "tgCf1YnMxFHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In AdaBoost, the weights of the misclassified samples are updated in order to focus the attention of the algorithm on the examples that are hardest to classify correctly. Specifically, the weight of each misclassified sample is increased by a factor that is proportional to the error rate of the weak learner at the current iteration."
      ],
      "metadata": {
        "id": "p1CtOSb3xIDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
      ],
      "metadata": {
        "id": "A_1fhlubxHg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved accuracy\n",
        "# Overfitting\n",
        "# Increased computational cost"
      ],
      "metadata": {
        "id": "onzA7keCxKPn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}